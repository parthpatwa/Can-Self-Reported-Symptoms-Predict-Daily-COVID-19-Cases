{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IJCAI_deep_learning_model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGDUcM5O3DNU"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36oK8ygt25tY"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, mean_squared_error,median_absolute_error,mean_absolute_error\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.svm import LinearSVC,LinearSVR\n",
        "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import permutations\n",
        "from itertools import combinations \n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey3Zz4ll3L_T",
        "outputId": "ca3fc5e9-97ea-4d17-8c39-c0720a24ce6e"
      },
      "source": [
        "# Load states data\n",
        "state_data = pd.DataFrame()\n",
        "\n",
        "# Insert relevant path here\n",
        "state_csv = '/content/drive/MyDrive/MIT_path_check/predict_daily_cases/state.csv'\n",
        "state_data = pd.read_csv(state_csv, low_memory=False)\n",
        "\n",
        "print(state_data.shape)\n",
        "state_data_dropped = state_data.dropna()# na values dropped # may be fill na \n",
        "print(state_data_dropped.shape)\n",
        "state_data_dropped = state_data_dropped[(state_data_dropped['gender']=='overall') & (state_data_dropped['age_bucket']=='overall')]\n",
        "print(state_data_dropped.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(83404, 104)\n",
            "(76790, 104)\n",
            "(7441, 104)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTziPT-N3SFB"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOIZb4wt3UGf",
        "outputId": "975c82fb-1d2e-4dab-9f09-125840a7b7af"
      },
      "source": [
        "y = state_data_dropped['pct_tested_and_positive']\n",
        "output_like_variables = ['pct_tested_and_positive','pct_tested_and_negative', 'pct_tested_no_result', 'pct_could_not_get_tested', 'pct_did_not_try_to_get_tested',\\\n",
        "                       'pct_tested_and_positive_weighted','pct_contact_covid_positive', 'pct_contact_covid_positive_weighted' ,\\\n",
        "                       'pct_tested_and_negative_weighted',\t'pct_tested_no_result_weighted',\t'pct_could_not_get_tested_weighted',\t'pct_did_not_try_to_get_tested_weighted'] \n",
        "                        \n",
        "X = state_data_dropped.drop(output_like_variables,axis=1)\n",
        "\n",
        "unweighted = ['n','weight_sums','pct_cli','pct_ili', 'pct_cli_anosmia_ageusia',\\\n",
        "        'pct_hh_cli', 'pct_cmnty_cli', 'pct_hh_fever', 'pct_hh_sore_throat',\\\n",
        "         'pct_hh_cough', 'pct_hh_shortness_of_breath','pct_hh_difficulty_breathing', 'mean_hh_cli_ct', 'mean_cmnty_cli_ct',\\\n",
        "         'pct_self_fever', 'pct_self_cough', 'pct_self_shortness_of_breath',\\\n",
        "         'pct_self_difficulty_breathing', 'pct_self_tiredness_or_exhaustion',\\\n",
        "         'pct_self_nasal_congestion', 'pct_self_runny_nose',\\\n",
        "         'pct_self_muscle_joint_aches', 'pct_self_sore_throat',\n",
        "       'pct_self_persistent_pain_pressure_in_chest',\\\n",
        "       'pct_self_nausea_vomiting', 'pct_self_diarrhea',\\\n",
        "       'pct_self_anosmia_ageusia', 'pct_self_other', 'pct_self_none_of_above',\\\n",
        "       'pct_self_multiple_symptoms', 'pct_worked_outside_home',\\\n",
        "       'pct_avoid_contact_all_or_most_time',\\\n",
        "       'mean_outside_hh_contact_at_work_ct',\\\n",
        "       'mean_outside_hh_contact_shopping_ct',\\\n",
        "       'mean_outside_hh_contact_in_social_gatherings_ct', 'pct_diabetes',\\\n",
        "       'pct_cancer', 'pct_heart_disease', 'pct_high_blood_pressure', \\\n",
        "       'pct_asthma', 'pct_chronic_lung_disease', 'pct_kidney_disease',\\\n",
        "       'pct_autoimmune_disorder', 'pct_no_above_medical_conditions',\\\n",
        "       'pct_multiple_medical_conditions', ]\n",
        "\n",
        "X = X.drop(unweighted,axis=1)\n",
        "n_features = X.shape[1]\n",
        "print(n_features, X.shape)\n",
        "\n",
        "derived = ['pct_ili_weighted','pct_cli_weighted','pct_cli_anosmia_ageusia_weighted',]\n",
        "X = X.drop(derived,axis=1)\n",
        "n_features = X.shape[1]\n",
        "\n",
        "mean_D = ['mean_outside_hh_contact_in_social_gatherings_ct_weighted','mean_cmnty_cli_ct_weighted',\\\n",
        "        'mean_outside_hh_contact_shopping_ct_weighted','mean_outside_hh_contact_at_work_ct_weighted']\n",
        "X = X.drop(mean_D,axis=1)\n",
        "n_features = X.shape[1]\n",
        "print(n_features, X.shape)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 (7441, 47)\n",
            "40 (7441, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRCdiOeF3VyX",
        "outputId": "b6e7abe8-1d6c-4fa7-f3e6-9380b5e134dc"
      },
      "source": [
        "who = pd.read_csv('/content/drive/MyDrive/MIT_path_check/predict_daily_cases/refined-us-states.csv')\n",
        "print(who.info())\n",
        "who = who.rename(columns={\"state\": \"state_code\"})\n",
        "\n",
        "who['date'] = list(map(lambda x : '-'.join(list(reversed(x.split('-')))) ,list(who['date'])) )\n",
        "print(who.head())\n",
        "\n",
        "\n",
        "merged = pd.merge(X, who,  how='inner', on=['date','state_code'] )\n",
        "merged = merged.sort_values(['state_code', 'date'], ascending=[True, True])\n",
        "print(len(merged))\n",
        "print('num states: ', merged['state_code'].nunique())\n",
        "merged.head()\n",
        "\n",
        "merged = merged.drop_duplicates(subset = ['state_code','date']) # duplicates need to be added instead of removing\n",
        "merged['daily_case'] = merged['cases'].diff().fillna(merged['cases'].iloc[0])\n",
        "merged.head()\n",
        "\n",
        "\n",
        "print(len(merged))\n",
        "merged = merged[ ~merged['date'].isin(['2020-04-16'])]\n",
        "print(len(merged))\n",
        "\n",
        "merged = merged.drop(['mean_hh_cli_ct_weighted'], axis =1)\n",
        "len(merged.columns)\n",
        "\n",
        "features = ['pct_cmnty_cli_weighted', 'pct_self_anosmia_ageusia_weighted', 'pct_hh_cli_weighted', 'pct_hh_fever_weighted', 'pct_self_fever_weighted', 'pct_hh_sore_throat_weighted', 'pct_avoid_contact_all_or_most_time_weighted', 'pct_hh_difficulty_breathing_weighted', 'pct_self_persistent_pain_pressure_in_chest_weighted', 'pct_self_runny_nose_weighted', 'pct_worked_outside_home_weighted', 'pct_self_nausea_vomiting_weighted', 'pct_hh_shortness_of_breath_weighted', 'pct_self_sore_throat_weighted', 'pct_self_difficulty_breathing_weighted', 'pct_asthma_weighted', 'pct_self_shortness_of_breath_weighted', 'pct_hh_cough_weighted', 'pct_self_none_of_above_weighted', 'pct_self_diarrhea_weighted', 'pct_chronic_lung_disease_weighted', 'pct_cancer_weighted', 'pct_self_other_weighted', 'pct_self_tiredness_or_exhaustion_weighted', 'pct_self_cough_weighted', 'pct_no_above_medical_conditions_weighted', 'pct_heart_disease_weighted', 'pct_multiple_medical_conditions_weighted', 'pct_autoimmune_disorder_weighted', 'pct_self_nasal_congestion_weighted', 'pct_kidney_disease_weighted',  'pct_self_multiple_symptoms_weighted', 'pct_self_muscle_joint_aches_weighted', 'pct_high_blood_pressure_weighted', 'pct_diabetes_weighted' ]\n",
        "len(features)\n",
        "y = merged['daily_case']\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13764 entries, 0 to 13763\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   date    13764 non-null  object\n",
            " 1   state   13764 non-null  object\n",
            " 2   cases   13764 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 322.7+ KB\n",
            "None\n",
            "         date state_code  cases\n",
            "0  2020-03-13         al      6\n",
            "1  2020-03-14         al     12\n",
            "2  2020-03-15         al     23\n",
            "3  2020-03-16         al     29\n",
            "4  2020-03-17         al     39\n",
            "7363\n",
            "num states:  50\n",
            "7228\n",
            "7178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_nZdmg3cwv"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLrUUEMN3eCx",
        "outputId": "b6928aad-6b36-4411-9bcc-56e265464716"
      },
      "source": [
        "features_ranked = []\n",
        "features_copy = features.copy()\n",
        "i = 1\n",
        "for fr in (features):\n",
        "    X_reg_new = SelectKBest(score_func=f_regression, k=1).fit_transform(merged[features_copy],y)\n",
        "    to_comp = np.squeeze(X_reg_new)\n",
        "    cols = features\n",
        "    for c in cols:\n",
        "        cur_col=np.array(merged[c])\n",
        "        if np.allclose(to_comp,cur_col):\n",
        "            selected_feature=c\n",
        "            features_ranked.append(selected_feature)\n",
        "            print(i,selected_feature)\n",
        "            features_copy.remove(selected_feature)\n",
        "            break\n",
        "    i +=1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 pct_cmnty_cli_weighted\n",
            "2 pct_self_anosmia_ageusia_weighted\n",
            "3 pct_hh_fever_weighted\n",
            "4 pct_hh_cli_weighted\n",
            "5 pct_hh_sore_throat_weighted\n",
            "6 pct_self_fever_weighted\n",
            "7 pct_asthma_weighted\n",
            "8 pct_autoimmune_disorder_weighted\n",
            "9 pct_self_runny_nose_weighted\n",
            "10 pct_self_sore_throat_weighted\n",
            "11 pct_avoid_contact_all_or_most_time_weighted\n",
            "12 pct_self_none_of_above_weighted\n",
            "13 pct_self_muscle_joint_aches_weighted\n",
            "14 pct_self_persistent_pain_pressure_in_chest_weighted\n",
            "15 pct_chronic_lung_disease_weighted\n",
            "16 pct_no_above_medical_conditions_weighted\n",
            "17 pct_self_nasal_congestion_weighted\n",
            "18 pct_self_multiple_symptoms_weighted\n",
            "19 pct_cancer_weighted\n",
            "20 pct_self_nausea_vomiting_weighted\n",
            "21 pct_multiple_medical_conditions_weighted\n",
            "22 pct_heart_disease_weighted\n",
            "23 pct_self_tiredness_or_exhaustion_weighted\n",
            "24 pct_diabetes_weighted\n",
            "25 pct_hh_difficulty_breathing_weighted\n",
            "26 pct_self_other_weighted\n",
            "27 pct_worked_outside_home_weighted\n",
            "28 pct_hh_shortness_of_breath_weighted\n",
            "29 pct_hh_cough_weighted\n",
            "30 pct_self_cough_weighted\n",
            "31 pct_self_shortness_of_breath_weighted\n",
            "32 pct_high_blood_pressure_weighted\n",
            "33 pct_self_diarrhea_weighted\n",
            "34 pct_self_difficulty_breathing_weighted\n",
            "35 pct_kidney_disease_weighted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taYtM4y53gl9"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DQkmLhNF3hyf",
        "outputId": "9b8e1166-287e-4381-cbbc-d0bca950e901"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, input_shape,  verbose=True, build=True):\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, )\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "      \n",
        "            self.model.save_weights('model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        n_feature_maps = 64 * 2\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_y)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        # r= 4 worked better\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=1, padding='same')(conv_y_1)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=3, padding='same')(conv_y_2)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=5, padding='same')(conv_y_3)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.add([conv_y_1, conv_y_2, conv_y_3])\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(128, activation='linear')(gap_layer)\n",
        "\n",
        "        output_layer = keras.layers.Dense(1, activation='linear')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='mae', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "        file_path =  'best_model.hdf5'\n",
        "\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint('model_{epoch:03d}_{mae:03f}_{val_mae:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(file_path, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "        #                                                   save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit_predict(self, x_train, y_train, x_val, y_val):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = 128\n",
        "        nb_epochs = 500\n",
        "\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save( 'best_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        start_time = time.time()\n",
        "        model_path =  'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        return y_pred\n",
        "\n",
        "merged = merged.sort_values(by='date', ascending=True)\n",
        "X_train = merged[:len(merged)*80//100]\n",
        "X_test = merged[len(merged)*80//100:]\n",
        "X_train = X_train.sample(frac=1)\n",
        "y_train = X_train['daily_case']\n",
        "X_test = X_test.sample(frac=1)\n",
        "y_test = X_test['daily_case']\n",
        "print(len(X_train), len(y_test))\n",
        "\n",
        "X_train = X_train[features_ranked]\n",
        "X_test = X_test[features_ranked]\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "\n",
        "model= Classifier_RESNET(input_shape=(1, 35))\n",
        "y_pred = model.fit_predict(X_train, y_train, X_test, y_test)\n",
        "\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_test.tolist()\n",
        "abs_sum = 0\n",
        "pct_sum = 0\n",
        "for idx in range(len(y_pred)):\n",
        "    diff = abs(yp_l[idx][0] - yt_l[idx])\n",
        "    abs_sum += diff\n",
        "    pct = diff/ (yt_l[idx] + 1)\n",
        "    pct_sm += pct\n",
        "\n",
        "rel_error = pct_sm / len(y_pred)\n",
        "abs_error = abs_sum / len(y_pred)\n",
        "gb_rel_errors.append(rel_error * 100)\n",
        "gb_abs_errors.append(abs_error)\n",
        "\n",
        "print('Test: ', abs_error,rel_error * 100)\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_train.tolist()\n",
        "\n",
        "abs_sum = 0\n",
        "pct_sum = 0\n",
        "for idx in range(len(y_pred)):\n",
        "    diff = abs(yp_l[idx][0] - yt_l[idx])\n",
        "    abs_sum += diff\n",
        "    pct = diff/ (yt_l[idx] + 1)\n",
        "    pct_sm += pct\n",
        "\n",
        "rel_error = pct_sm / len(y_pred)\n",
        "abs_error = abs_sum / len(y_pred)\n",
        "gb_rel_errors.append(rel_error * 100)\n",
        "gb_abs_errors.append(abs_error)\n",
        "\n",
        "print('Train: ', abs_error,rel_error * 100)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5742 1436\n",
            "(5742, 35) (1436, 35)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1, 35)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 1, 128)       35968       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 1, 128)       512         conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 1, 128)       0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 1, 128)       49280       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 1, 128)       512         conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 1, 128)       0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 1, 128)       4608        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 1, 128)       49280       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 1, 128)       512         conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 1, 128)       512         conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 1, 128)       0           batch_normalization_21[0][0]     \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 1, 128)       0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 1, 256)       98560       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 1, 256)       1024        conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 1, 256)       0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 1, 16)        4112        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 1, 16)        64          conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 1, 16)        0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 1, 16)        272         activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 1, 16)        64          conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 1, 16)        0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 1, 256)       33024       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 1, 256)       12544       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 1, 256)       1024        conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 1, 256)       1024        conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 1, 256)       0           batch_normalization_26[0][0]     \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 1, 256)       0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 1, 256)       524544      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 1, 256)       1024        conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 1, 256)       0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 1, 16)        4112        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 1, 16)        12304       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 1, 16)        20496       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 1, 16)        64          conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 1, 16)        64          conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 1, 16)        64          conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 1, 16)        0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 1, 16)        0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 1, 16)        0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 1, 64)        1088        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 1, 64)        3136        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 1, 64)        5184        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 1, 64)        256         conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 1, 64)        256         conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 1, 64)        256         conv1d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 1, 64)        0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 1, 64)        0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 1, 64)        0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 1, 64)        0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 1, 256)       49408       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 1, 256)       1024        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 1, 256)       1024        conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 1, 256)       0           batch_normalization_35[0][0]     \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 1, 256)       0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 256)          0           activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 256)          0           global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          65792       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,016,017\n",
            "Trainable params: 1,011,377\n",
            "Non-trainable params: 4,640\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "45/45 [==============================] - 13s 190ms/step - loss: 568.8125 - mae: 568.8125 - val_loss: 1573.2758 - val_mae: 1573.2758\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1573.27576, saving model to best_model.hdf5\n",
            "Epoch 2/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 8s 177ms/step - loss: 368.3604 - mae: 368.3604 - val_loss: 641.7217 - val_mae: 641.7217\n",
            "\n",
            "Epoch 00002: val_loss improved from 1573.27576 to 641.72174, saving model to best_model.hdf5\n",
            "Epoch 3/500\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 339.3655 - mae: 339.3655 - val_loss: 737.8402 - val_mae: 737.8402\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 641.72174\n",
            "Epoch 4/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 307.7103 - mae: 307.7103 - val_loss: 787.9048 - val_mae: 787.9048\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 641.72174\n",
            "Epoch 5/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 297.1655 - mae: 297.1655 - val_loss: 750.3792 - val_mae: 750.3792\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 641.72174\n",
            "Epoch 6/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 287.3686 - mae: 287.3686 - val_loss: 792.0226 - val_mae: 792.0226\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 641.72174\n",
            "Epoch 7/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 296.0238 - mae: 296.0238 - val_loss: 757.7718 - val_mae: 757.7718\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 641.72174\n",
            "Epoch 8/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 272.0898 - mae: 272.0898 - val_loss: 726.3784 - val_mae: 726.3784\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 641.72174\n",
            "Epoch 9/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 266.8644 - mae: 266.8644 - val_loss: 518.6460 - val_mae: 518.6460\n",
            "\n",
            "Epoch 00009: val_loss improved from 641.72174 to 518.64600, saving model to best_model.hdf5\n",
            "Epoch 10/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 278.2929 - mae: 278.2929 - val_loss: 705.5009 - val_mae: 705.5009\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 518.64600\n",
            "Epoch 11/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 266.6029 - mae: 266.6029 - val_loss: 537.1147 - val_mae: 537.1147\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 518.64600\n",
            "Epoch 12/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 275.5299 - mae: 275.5299 - val_loss: 460.8901 - val_mae: 460.8901\n",
            "\n",
            "Epoch 00012: val_loss improved from 518.64600 to 460.89011, saving model to best_model.hdf5\n",
            "Epoch 13/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 259.6570 - mae: 259.6570 - val_loss: 569.6771 - val_mae: 569.6771\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 460.89011\n",
            "Epoch 14/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 262.2101 - mae: 262.2101 - val_loss: 429.7078 - val_mae: 429.7078\n",
            "\n",
            "Epoch 00014: val_loss improved from 460.89011 to 429.70782, saving model to best_model.hdf5\n",
            "Epoch 15/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 255.9485 - mae: 255.9485 - val_loss: 522.0471 - val_mae: 522.0471\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 429.70782\n",
            "Epoch 16/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 252.2795 - mae: 252.2795 - val_loss: 716.0970 - val_mae: 716.0970\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 429.70782\n",
            "Epoch 17/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 244.3948 - mae: 244.3948 - val_loss: 749.9246 - val_mae: 749.9246\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 429.70782\n",
            "Epoch 18/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 260.2551 - mae: 260.2551 - val_loss: 543.2153 - val_mae: 543.2153\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 429.70782\n",
            "Epoch 19/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 245.1102 - mae: 245.1102 - val_loss: 712.1127 - val_mae: 712.1127\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 429.70782\n",
            "Epoch 20/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 243.5677 - mae: 243.5677 - val_loss: 605.9891 - val_mae: 605.9891\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 429.70782\n",
            "Epoch 21/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 228.2865 - mae: 228.2865 - val_loss: 527.0745 - val_mae: 527.0745\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 429.70782\n",
            "Epoch 22/500\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 255.8574 - mae: 255.8574 - val_loss: 445.6632 - val_mae: 445.6632\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 429.70782\n",
            "Epoch 23/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 238.9506 - mae: 238.9506 - val_loss: 580.7609 - val_mae: 580.7609\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 429.70782\n",
            "Epoch 24/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 230.2831 - mae: 230.2831 - val_loss: 608.5307 - val_mae: 608.5307\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 429.70782\n",
            "Epoch 25/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 235.1993 - mae: 235.1993 - val_loss: 531.8898 - val_mae: 531.8898\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 429.70782\n",
            "Epoch 26/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 235.3994 - mae: 235.3994 - val_loss: 624.1035 - val_mae: 624.1035\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 429.70782\n",
            "Epoch 27/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 239.7011 - mae: 239.7011 - val_loss: 418.3778 - val_mae: 418.3778\n",
            "\n",
            "Epoch 00027: val_loss improved from 429.70782 to 418.37778, saving model to best_model.hdf5\n",
            "Epoch 28/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 224.4987 - mae: 224.4987 - val_loss: 459.4522 - val_mae: 459.4522\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 418.37778\n",
            "Epoch 29/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 241.6810 - mae: 241.6810 - val_loss: 445.5417 - val_mae: 445.5417\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 418.37778\n",
            "Epoch 30/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 227.2396 - mae: 227.2396 - val_loss: 488.3036 - val_mae: 488.3036\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 418.37778\n",
            "Epoch 31/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 228.8238 - mae: 228.8238 - val_loss: 411.0027 - val_mae: 411.0027\n",
            "\n",
            "Epoch 00031: val_loss improved from 418.37778 to 411.00269, saving model to best_model.hdf5\n",
            "Epoch 32/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 225.5987 - mae: 225.5987 - val_loss: 478.4123 - val_mae: 478.4123\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 411.00269\n",
            "Epoch 33/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 228.8221 - mae: 228.8221 - val_loss: 591.2533 - val_mae: 591.2533\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 411.00269\n",
            "Epoch 34/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 218.0502 - mae: 218.0502 - val_loss: 611.9310 - val_mae: 611.9310\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 411.00269\n",
            "Epoch 35/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 223.6305 - mae: 223.6305 - val_loss: 655.2209 - val_mae: 655.2209\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 411.00269\n",
            "Epoch 36/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 220.7064 - mae: 220.7064 - val_loss: 596.7966 - val_mae: 596.7966\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 411.00269\n",
            "Epoch 37/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 225.3842 - mae: 225.3842 - val_loss: 536.8513 - val_mae: 536.8513\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 411.00269\n",
            "Epoch 38/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 221.1663 - mae: 221.1663 - val_loss: 423.6692 - val_mae: 423.6692\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 411.00269\n",
            "Epoch 39/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 212.1774 - mae: 212.1774 - val_loss: 507.3185 - val_mae: 507.3185\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 411.00269\n",
            "Epoch 40/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 211.6565 - mae: 211.6565 - val_loss: 537.5712 - val_mae: 537.5712\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 411.00269\n",
            "Epoch 41/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 212.2318 - mae: 212.2318 - val_loss: 543.9622 - val_mae: 543.9622\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 411.00269\n",
            "Epoch 42/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 202.7357 - mae: 202.7357 - val_loss: 520.6551 - val_mae: 520.6551\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 411.00269\n",
            "Epoch 43/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 216.7474 - mae: 216.7474 - val_loss: 679.0417 - val_mae: 679.0417\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 411.00269\n",
            "Epoch 44/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 210.7706 - mae: 210.7706 - val_loss: 439.1426 - val_mae: 439.1426\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 411.00269\n",
            "Epoch 45/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 213.3054 - mae: 213.3054 - val_loss: 503.8580 - val_mae: 503.8580\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 411.00269\n",
            "Epoch 46/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 204.8752 - mae: 204.8752 - val_loss: 487.4328 - val_mae: 487.4328\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 411.00269\n",
            "Epoch 47/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 200.6480 - mae: 200.6480 - val_loss: 469.9460 - val_mae: 469.9460\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 411.00269\n",
            "Epoch 48/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 211.5203 - mae: 211.5203 - val_loss: 472.1842 - val_mae: 472.1842\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 411.00269\n",
            "Epoch 49/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 206.4686 - mae: 206.4686 - val_loss: 525.5013 - val_mae: 525.5013\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 411.00269\n",
            "Epoch 50/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 195.6504 - mae: 195.6504 - val_loss: 460.0319 - val_mae: 460.0319\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 411.00269\n",
            "Epoch 51/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 194.4848 - mae: 194.4848 - val_loss: 649.1619 - val_mae: 649.1619\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 411.00269\n",
            "Epoch 52/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 205.6117 - mae: 205.6117 - val_loss: 492.2411 - val_mae: 492.2411\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 411.00269\n",
            "Epoch 53/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 199.8107 - mae: 199.8107 - val_loss: 541.1848 - val_mae: 541.1848\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 411.00269\n",
            "Epoch 54/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 182.5861 - mae: 182.5861 - val_loss: 404.4837 - val_mae: 404.4837\n",
            "\n",
            "Epoch 00054: val_loss improved from 411.00269 to 404.48367, saving model to best_model.hdf5\n",
            "Epoch 55/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 192.3531 - mae: 192.3531 - val_loss: 435.6675 - val_mae: 435.6675\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 404.48367\n",
            "Epoch 56/500\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 188.0336 - mae: 188.0336 - val_loss: 459.5022 - val_mae: 459.5022\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 404.48367\n",
            "Epoch 57/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 191.9677 - mae: 191.9677 - val_loss: 456.8565 - val_mae: 456.8565\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 404.48367\n",
            "Epoch 58/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 188.7247 - mae: 188.7247 - val_loss: 509.6035 - val_mae: 509.6035\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 404.48367\n",
            "Epoch 59/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 185.6826 - mae: 185.6826 - val_loss: 426.7832 - val_mae: 426.7832\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 404.48367\n",
            "Epoch 60/500\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 193.6482 - mae: 193.6482 - val_loss: 377.2000 - val_mae: 377.2000\n",
            "\n",
            "Epoch 00060: val_loss improved from 404.48367 to 377.19995, saving model to best_model.hdf5\n",
            "Epoch 61/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 187.4978 - mae: 187.4978 - val_loss: 512.5811 - val_mae: 512.5811\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 377.19995\n",
            "Epoch 62/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 181.5951 - mae: 181.5951 - val_loss: 510.4625 - val_mae: 510.4625\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 377.19995\n",
            "Epoch 63/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 185.0690 - mae: 185.0690 - val_loss: 651.4537 - val_mae: 651.4537\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 377.19995\n",
            "Epoch 64/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 182.7769 - mae: 182.7769 - val_loss: 453.4663 - val_mae: 453.4663\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 377.19995\n",
            "Epoch 65/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 186.1649 - mae: 186.1649 - val_loss: 497.7795 - val_mae: 497.7795\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 377.19995\n",
            "Epoch 66/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 185.4299 - mae: 185.4299 - val_loss: 479.5054 - val_mae: 479.5054\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 377.19995\n",
            "Epoch 67/500\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 187.7990 - mae: 187.7990 - val_loss: 492.3423 - val_mae: 492.3423\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 377.19995\n",
            "Epoch 68/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 181.1322 - mae: 181.1322 - val_loss: 477.6980 - val_mae: 477.6980\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 377.19995\n",
            "Epoch 69/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 190.1937 - mae: 190.1937 - val_loss: 588.5260 - val_mae: 588.5260\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 377.19995\n",
            "Epoch 70/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 181.2369 - mae: 181.2369 - val_loss: 703.8833 - val_mae: 703.8833\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 377.19995\n",
            "Epoch 71/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 179.0121 - mae: 179.0121 - val_loss: 588.6015 - val_mae: 588.6015\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 377.19995\n",
            "Epoch 72/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 181.1650 - mae: 181.1650 - val_loss: 574.9916 - val_mae: 574.9916\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 377.19995\n",
            "Epoch 73/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 177.7406 - mae: 177.7406 - val_loss: 498.4939 - val_mae: 498.4939\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 377.19995\n",
            "Epoch 74/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 170.8372 - mae: 170.8372 - val_loss: 385.2076 - val_mae: 385.2076\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 377.19995\n",
            "Epoch 75/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 170.0648 - mae: 170.0648 - val_loss: 547.6596 - val_mae: 547.6596\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 377.19995\n",
            "Epoch 76/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 172.4662 - mae: 172.4662 - val_loss: 495.4694 - val_mae: 495.4694\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 377.19995\n",
            "Epoch 77/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 173.5508 - mae: 173.5508 - val_loss: 598.8546 - val_mae: 598.8546\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 377.19995\n",
            "Epoch 78/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 167.1738 - mae: 167.1738 - val_loss: 563.4090 - val_mae: 563.4090\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 377.19995\n",
            "Epoch 79/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 170.7106 - mae: 170.7106 - val_loss: 502.3362 - val_mae: 502.3362\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 377.19995\n",
            "Epoch 80/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 181.3346 - mae: 181.3346 - val_loss: 393.6950 - val_mae: 393.6950\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 377.19995\n",
            "Epoch 81/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 173.2204 - mae: 173.2204 - val_loss: 387.2262 - val_mae: 387.2262\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 377.19995\n",
            "Epoch 82/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 177.8582 - mae: 177.8582 - val_loss: 407.9100 - val_mae: 407.9100\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 377.19995\n",
            "Epoch 83/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 172.4401 - mae: 172.4401 - val_loss: 425.5106 - val_mae: 425.5106\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 377.19995\n",
            "Epoch 84/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 167.0510 - mae: 167.0510 - val_loss: 462.3370 - val_mae: 462.3370\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 377.19995\n",
            "Epoch 85/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 162.8513 - mae: 162.8513 - val_loss: 481.8427 - val_mae: 481.8427\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 377.19995\n",
            "Epoch 86/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 159.9290 - mae: 159.9290 - val_loss: 566.1182 - val_mae: 566.1182\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 377.19995\n",
            "Epoch 87/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 167.3352 - mae: 167.3352 - val_loss: 534.4626 - val_mae: 534.4626\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 377.19995\n",
            "Epoch 88/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 164.4703 - mae: 164.4703 - val_loss: 453.2000 - val_mae: 453.2000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 377.19995\n",
            "Epoch 89/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 165.6502 - mae: 165.6502 - val_loss: 407.8694 - val_mae: 407.8694\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 377.19995\n",
            "Epoch 90/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 159.9436 - mae: 159.9436 - val_loss: 405.8944 - val_mae: 405.8944\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 377.19995\n",
            "Epoch 91/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 165.3364 - mae: 165.3364 - val_loss: 510.9582 - val_mae: 510.9582\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 377.19995\n",
            "Epoch 92/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 156.5421 - mae: 156.5421 - val_loss: 439.3337 - val_mae: 439.3337\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 377.19995\n",
            "Epoch 93/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 158.9946 - mae: 158.9946 - val_loss: 538.3408 - val_mae: 538.3408\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 377.19995\n",
            "Epoch 94/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 163.8858 - mae: 163.8858 - val_loss: 519.3772 - val_mae: 519.3772\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 377.19995\n",
            "Epoch 95/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 161.1897 - mae: 161.1897 - val_loss: 440.4984 - val_mae: 440.4984\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 377.19995\n",
            "Epoch 96/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 161.4108 - mae: 161.4108 - val_loss: 387.5602 - val_mae: 387.5602\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 377.19995\n",
            "Epoch 97/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 158.9094 - mae: 158.9094 - val_loss: 481.5323 - val_mae: 481.5323\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 377.19995\n",
            "Epoch 98/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 155.4064 - mae: 155.4064 - val_loss: 547.4908 - val_mae: 547.4908\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 377.19995\n",
            "Epoch 99/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 155.2350 - mae: 155.2350 - val_loss: 519.2987 - val_mae: 519.2987\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 377.19995\n",
            "Epoch 100/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 149.3817 - mae: 149.3817 - val_loss: 478.8714 - val_mae: 478.8714\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 377.19995\n",
            "Epoch 101/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 146.7507 - mae: 146.7507 - val_loss: 591.7176 - val_mae: 591.7176\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 377.19995\n",
            "Epoch 102/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 149.6944 - mae: 149.6944 - val_loss: 535.8812 - val_mae: 535.8812\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 377.19995\n",
            "Epoch 103/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 146.0430 - mae: 146.0430 - val_loss: 459.2868 - val_mae: 459.2868\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 377.19995\n",
            "Epoch 104/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 151.0889 - mae: 151.0889 - val_loss: 394.4676 - val_mae: 394.4676\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 377.19995\n",
            "Epoch 105/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 154.6776 - mae: 154.6776 - val_loss: 455.1237 - val_mae: 455.1237\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 377.19995\n",
            "Epoch 106/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 151.7273 - mae: 151.7273 - val_loss: 443.2813 - val_mae: 443.2813\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 377.19995\n",
            "Epoch 107/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 155.9333 - mae: 155.9333 - val_loss: 386.6325 - val_mae: 386.6325\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 377.19995\n",
            "Epoch 108/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 154.0467 - mae: 154.0467 - val_loss: 599.3021 - val_mae: 599.3021\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 377.19995\n",
            "Epoch 109/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 147.5625 - mae: 147.5625 - val_loss: 473.4409 - val_mae: 473.4409\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 377.19995\n",
            "Epoch 110/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 153.7450 - mae: 153.7450 - val_loss: 496.6825 - val_mae: 496.6825\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 377.19995\n",
            "Epoch 111/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 147.1224 - mae: 147.1224 - val_loss: 445.8166 - val_mae: 445.8166\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 377.19995\n",
            "Epoch 112/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 149.1028 - mae: 149.1028 - val_loss: 419.2070 - val_mae: 419.2070\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 377.19995\n",
            "Epoch 113/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 149.2599 - mae: 149.2599 - val_loss: 443.1133 - val_mae: 443.1133\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 377.19995\n",
            "Epoch 114/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 135.2135 - mae: 135.2135 - val_loss: 426.1404 - val_mae: 426.1404\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 377.19995\n",
            "Epoch 115/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 138.0714 - mae: 138.0714 - val_loss: 430.6680 - val_mae: 430.6680\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 377.19995\n",
            "Epoch 116/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 136.1828 - mae: 136.1828 - val_loss: 405.1279 - val_mae: 405.1279\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 377.19995\n",
            "Epoch 117/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 130.9588 - mae: 130.9588 - val_loss: 410.7621 - val_mae: 410.7621\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 377.19995\n",
            "Epoch 118/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 138.5274 - mae: 138.5274 - val_loss: 415.4220 - val_mae: 415.4220\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 377.19995\n",
            "Epoch 119/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 135.8761 - mae: 135.8761 - val_loss: 368.9722 - val_mae: 368.9722\n",
            "\n",
            "Epoch 00119: val_loss improved from 377.19995 to 368.97223, saving model to best_model.hdf5\n",
            "Epoch 120/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 131.7489 - mae: 131.7489 - val_loss: 398.1391 - val_mae: 398.1391\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 368.97223\n",
            "Epoch 121/500\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 130.7337 - mae: 130.7337 - val_loss: 367.6359 - val_mae: 367.6359\n",
            "\n",
            "Epoch 00121: val_loss improved from 368.97223 to 367.63589, saving model to best_model.hdf5\n",
            "Epoch 122/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 123.6043 - mae: 123.6043 - val_loss: 435.3099 - val_mae: 435.3099\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 367.63589\n",
            "Epoch 123/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 128.2409 - mae: 128.2409 - val_loss: 455.8178 - val_mae: 455.8178\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 367.63589\n",
            "Epoch 124/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 133.7499 - mae: 133.7499 - val_loss: 490.9319 - val_mae: 490.9319\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 367.63589\n",
            "Epoch 125/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 126.7762 - mae: 126.7762 - val_loss: 381.4760 - val_mae: 381.4760\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 367.63589\n",
            "Epoch 126/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 126.5402 - mae: 126.5402 - val_loss: 410.7263 - val_mae: 410.7263\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 367.63589\n",
            "Epoch 127/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 129.7904 - mae: 129.7904 - val_loss: 401.1489 - val_mae: 401.1489\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 367.63589\n",
            "Epoch 128/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 126.0484 - mae: 126.0484 - val_loss: 433.0204 - val_mae: 433.0204\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 367.63589\n",
            "Epoch 129/500\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 123.0789 - mae: 123.0789 - val_loss: 353.8964 - val_mae: 353.8964\n",
            "\n",
            "Epoch 00129: val_loss improved from 367.63589 to 353.89636, saving model to best_model.hdf5\n",
            "Epoch 130/500\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 131.6699 - mae: 131.6699 - val_loss: 375.6732 - val_mae: 375.6732\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 353.89636\n",
            "Epoch 131/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 126.0899 - mae: 126.0899 - val_loss: 379.1809 - val_mae: 379.1809\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 353.89636\n",
            "Epoch 132/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 123.7319 - mae: 123.7319 - val_loss: 412.4414 - val_mae: 412.4414\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 353.89636\n",
            "Epoch 133/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 128.4274 - mae: 128.4274 - val_loss: 425.9522 - val_mae: 425.9522\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 353.89636\n",
            "Epoch 134/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 128.3747 - mae: 128.3747 - val_loss: 389.3427 - val_mae: 389.3427\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 353.89636\n",
            "Epoch 135/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 125.8939 - mae: 125.8939 - val_loss: 407.9637 - val_mae: 407.9637\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 353.89636\n",
            "Epoch 136/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 124.6052 - mae: 124.6052 - val_loss: 369.5710 - val_mae: 369.5710\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 353.89636\n",
            "Epoch 137/500\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 130.6979 - mae: 130.6979 - val_loss: 428.9501 - val_mae: 428.9501\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 353.89636\n",
            "Epoch 138/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 129.6434 - mae: 129.6434 - val_loss: 392.7503 - val_mae: 392.7503\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 353.89636\n",
            "Epoch 139/500\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 123.7648 - mae: 123.7648 - val_loss: 444.5934 - val_mae: 444.5934\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 353.89636\n",
            "Epoch 140/500\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 118.6410 - mae: 118.6410 - val_loss: 414.5195 - val_mae: 414.5195\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 353.89636\n",
            "Epoch 141/500\n",
            "17/45 [==========>...................] - ETA: 4s - loss: 123.1567 - mae: 123.1567"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-25df32ee7ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mClassifier_RESNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0myp_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-25df32ee7ec3>\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n\u001b[0;32m--> 157\u001b[0;31m                               verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1of1EDkq9uhh"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmc1j9Bt4Ixf",
        "outputId": "fb08e7b7-cb1f-4ba4-bf73-f0deddf665b1"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, input_shape,  verbose=True, build=True):\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, )\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "      \n",
        "            self.model.save_weights('model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        n_feature_maps = 64 * 2\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_y)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        # r= 4 worked better\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=1, padding='same')(conv_y_1)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=3, padding='same')(conv_y_2)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=5, padding='same')(conv_y_3)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.add([conv_y_1, conv_y_2, conv_y_3])\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(128, activation='linear')(gap_layer)\n",
        "\n",
        "        output_layer = keras.layers.Dense(1, activation='linear')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='mae', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "        file_path =  'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint('model_{epoch:03d}_{mae:03f}_{val_mae:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "        #                                                   save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit_predict(self, x_train, y_train, x_val, y_val):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = 128\n",
        "        nb_epochs = 500\n",
        "\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save( 'best_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        start_time = time.time()\n",
        "        model_path =  'best_model.hdf5'\n",
        "        # model_path = '/content/model_045_204.676987_362.147278.h5'\n",
        "        # model_path = '/content/model_118_138.541870_367.545013.h5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model= Classifier_RESNET(input_shape=(1, 35))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 1, 35)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 1, 128)       35968       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 1, 128)       512         conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 1, 128)       0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 1, 128)       49280       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 1, 128)       512         conv1d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 1, 128)       0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 1, 128)       4608        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 1, 128)       49280       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 1, 128)       512         conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 1, 128)       512         conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 1, 128)       0           batch_normalization_39[0][0]     \n",
            "                                                                 batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 1, 128)       0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 1, 256)       98560       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 1, 256)       1024        conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 1, 256)       0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 1, 16)        4112        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 1, 16)        64          conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 1, 16)        0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 1, 16)        272         activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 1, 16)        64          conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 1, 16)        0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_42 (Conv1D)              (None, 1, 256)       33024       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 1, 256)       12544       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 1, 256)       1024        conv1d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 1, 256)       1024        conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 1, 256)       0           batch_normalization_44[0][0]     \n",
            "                                                                 batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 1, 256)       0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_43 (Conv1D)              (None, 1, 256)       524544      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 1, 256)       1024        conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 1, 256)       0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_44 (Conv1D)              (None, 1, 16)        4112        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_46 (Conv1D)              (None, 1, 16)        12304       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_48 (Conv1D)              (None, 1, 16)        20496       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 1, 16)        64          conv1d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 1, 16)        64          conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 1, 16)        64          conv1d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 1, 16)        0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 1, 16)        0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 1, 16)        0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_45 (Conv1D)              (None, 1, 64)        1088        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_47 (Conv1D)              (None, 1, 64)        3136        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_49 (Conv1D)              (None, 1, 64)        5184        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 1, 64)        256         conv1d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 1, 64)        256         conv1d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 1, 64)        256         conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 1, 64)        0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 1, 64)        0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 1, 64)        0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 1, 64)        0           activation_39[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_50 (Conv1D)              (None, 1, 256)       49408       add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 1, 256)       1024        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 1, 256)       1024        conv1d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 1, 256)       0           batch_normalization_53[0][0]     \n",
            "                                                                 batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 1, 256)       0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 256)          0           activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 256)          0           global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          65792       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,016,017\n",
            "Trainable params: 1,011,377\n",
            "Non-trainable params: 4,640\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN_cXOTH9Ahk",
        "outputId": "795fd8cb-171a-4ab3-c8e7-a0759c348f57"
      },
      "source": [
        "# Test scores results are better than the paper\n",
        "def get_mae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    abs_error = abs_sum/len(y_pred)\n",
        "    print('mae: ', abs_error)\n",
        "    return abs_error\n",
        "\n",
        "def get_nmae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    n_abs_error = abs_sum/sum(y_true)\n",
        "    print('nmae: ', n_abs_error)\n",
        "    return n_abs_error\n",
        "\n",
        "def get_mre(y_pred, y_true):\n",
        "    rel_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff= y_pred[j][0]-y_true[j]\n",
        "        rel = diff/(y_true[j] +1)\n",
        "        rel_sum += abs(rel)\n",
        "    rel_error = rel_sum/len(y_pred)\n",
        "    print('mre: ', rel_error)\n",
        "    return rel_error*100\n",
        "    \n",
        "print('Test: ')\n",
        "y_pred = model.predict(X_test)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_test.tolist()\n",
        "\n",
        "get_mae(yp_l, yt_l)\n",
        "get_nmae(yp_l, yt_l)\n",
        "get_mre(yp_l, yt_l)\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_train.tolist()\n",
        "\n",
        "print('Train: ')\n",
        "get_mae(yp_l, yt_l)\n",
        "get_nmae(yp_l, yt_l)\n",
        "get_mre(yp_l, yt_l)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: \n",
            "mae:  353.89634976181145\n",
            "nmae:  0.4236068089966552\n",
            "mre:  2.2123837190399915\n",
            "Train: \n",
            "mae:  117.47233944502246\n",
            "nmae:  0.15587882390196792\n",
            "mre:  1.5160377809062364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151.60377809062365"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ROfKnt9nch"
      },
      "source": [
        "### Global model evaluation on individual states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7jaK_1s9q3b",
        "outputId": "8f7ed0bf-356e-482d-f28e-c2c3eb73dacf"
      },
      "source": [
        "gdbt_statewise_abs_error_list = []\n",
        "gdbt_statewise_n_abs_error_list = []\n",
        "feature_importance_list_gdbt = []\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "statewise_dl_errors = []\n",
        "states = list(merged['state_code'].unique())\n",
        "states.sort()\n",
        "\n",
        "merged = merged.sort_values(by='date', ascending=True)\n",
        "X_train = merged[:len(merged)*80//100]\n",
        "X_test = merged[len(merged)*80//100:]\n",
        "X_train = X_train.sample(frac=1)\n",
        "y_train = X_train['daily_case']\n",
        "X_test = X_test.sample(frac=1)\n",
        "y_test = X_test['daily_case']\n",
        "print(len(X_train), len(y_test))\n",
        "\n",
        "# X_train = X_train[features_ranked]\n",
        "# X_test = X_test[features_ranked]\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "\n",
        "for state in states:\n",
        "    X_train_state = X_train[X_train['state_code'] ==state][features_ranked]\n",
        "    y_train_state =  X_train[X_train['state_code'] ==state]['daily_case']\n",
        "    X_test_state = X_test[X_test['state_code'] ==state][features_ranked]\n",
        "    y_test_state =  X_test[X_test['state_code'] ==state]['daily_case']   \n",
        "\n",
        "    X_train_state = np.expand_dims(X_train_state, axis=1)\n",
        "    X_test_state = np.expand_dims(X_test_state, axis=1)\n",
        "\n",
        "    # reg = GradientBoostingRegressor() \n",
        "    reg = Classifier_RESNET(input_shape=(1, 35), verbose=False)\n",
        "    # reg.fit_predict(X_train_state,y_train_state, X_test_state, y_test_state)    \n",
        "    y_pred= reg.predict(X_test_state)\n",
        "    yp_l=list(y_pred)\n",
        "    yt_l=list(y_test_state)\n",
        "\n",
        "    mae  = get_mae(yp_l,yt_l)\n",
        "    nmae = get_nmae(yp_l,yt_l) \n",
        "    mre  = get_mre(yp_l,yt_l)    \n",
        "\n",
        "    print(state,mae,nmae,mre, end ='\\t') \n",
        "    statewise_dl_errors.append([[state, mae, nmae, mre]])\n",
        "\n",
        "    print ()    \n",
        "    gdbt_statewise_abs_error_list.append((state,mae,len(y_test_state)))\n",
        "    gdbt_statewise_n_abs_error_list.append((state,nmae,sum(y_test_state)))    \n",
        "    # importances = reg.feature_importances_\n",
        "    # for i in range(len(features_ranked)):\n",
        "    #    feature_importance_list_gdbt.append([state,features_ranked[i],importances[i]])sum_error = 0\n",
        "\n",
        "sum_support = 0\n",
        "sum_error = 0\n",
        "for tpl in gdbt_statewise_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "\n",
        "state_model_global_error = sum_error/sum_support\n",
        "sum_error = 0\n",
        "sum_support = 0\n",
        "for tpl in gdbt_statewise_n_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "state_model_global_error_norm = sum_error/sum_support\n",
        "\n",
        "print('global level', state_model_global_error,state_model_global_error_norm)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5742 1436\n",
            "(5742, 41) (1436, 41)\n",
            "mae:  34.93873430553236\n",
            "nmae:  0.4731546342160476\n",
            "mre:  0.7096196168140733\n",
            "ak 34.93873430553236 0.4731546342160476 70.96196168140733\t\n",
            "mae:  555.7893218994141\n",
            "nmae:  0.5064503916401548\n",
            "mre:  0.5197546683620097\n",
            "al 555.7893218994141 0.5064503916401548 51.975466836200965\t\n",
            "mae:  426.6389475370708\n",
            "nmae:  0.7947196081572888\n",
            "mre:  0.7671419284304777\n",
            "ar 426.6389475370708 0.7947196081572888 76.71419284304777\t\n",
            "WARNING:tensorflow:5 out of the last 184 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f35ef561c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "mae:  433.0500231291118\n",
            "nmae:  0.6046406848510527\n",
            "mre:  0.9428599901392624\n",
            "az 433.0500231291118 0.6046406848510527 94.28599901392623\t\n",
            "WARNING:tensorflow:6 out of the last 185 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f35f4ee2200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "mae:  1742.101870888158\n",
            "nmae:  0.24492164376688003\n",
            "mre:  0.20804268261237788\n",
            "ca 1742.101870888158 0.24492164376688003 20.804268261237787\t\n",
            "mae:  124.18434102911698\n",
            "nmae:  0.371049296989027\n",
            "mre:  0.378524632022131\n",
            "co 124.18434102911698 0.371049296989027 37.8524632022131\t\n",
            "mae:  27.97583096822103\n",
            "nmae:  0.5216127588854138\n",
            "mre:  0.5192127335054593\n",
            "dc 27.97583096822103 0.5216127588854138 51.921273350545924\t\n",
            "mae:  63.467776711781816\n",
            "nmae:  0.6966825105574295\n",
            "mre:  1.1303187162117552\n",
            "de 63.467776711781816 0.6966825105574295 113.03187162117551\t\n",
            "mae:  1304.4033880869547\n",
            "nmae:  0.33364682607138657\n",
            "mre:  0.35022123870079996\n",
            "fl 1304.4033880869547 0.33364682607138657 35.022123870079994\t\n",
            "mae:  721.2096433003743\n",
            "nmae:  0.31568772048690824\n",
            "mre:  0.3910276855212912\n",
            "ga 721.2096433003743 0.31568772048690824 39.10276855212912\t\n",
            "mae:  278.16956186294556\n",
            "nmae:  1.2539574539276284\n",
            "mre:  1.041080799256641\n",
            "hi 278.16956186294556 1.2539574539276284 104.10807992566411\t\n",
            "mae:  338.92837041219076\n",
            "nmae:  0.460104579952293\n",
            "mre:  0.4211843296431758\n",
            "ia 338.92837041219076 0.460104579952293 42.118432964317584\t\n",
            "mae:  179.01713180541992\n",
            "nmae:  0.5557237121443085\n",
            "mre:  0.6617865262302561\n",
            "id 179.01713180541992 0.5557237121443085 66.17865262302561\t\n",
            "mae:  836.6678526560465\n",
            "nmae:  0.4230438141252848\n",
            "mre:  0.3784493191986497\n",
            "il 836.6678526560465 0.4230438141252848 37.84493191986497\t\n",
            "mae:  350.3296829223633\n",
            "nmae:  0.39148813557591067\n",
            "mre:  0.4134231032481529\n",
            "in 350.3296829223633 0.39148813557591067 41.34231032481529\t\n",
            "mae:  439.7833652496338\n",
            "nmae:  0.7884719391315971\n",
            "mre:  2.6162751787773604\n",
            "ks 439.7833652496338 0.7884719391315971 261.627517877736\t\n",
            "mae:  353.50369415283205\n",
            "nmae:  0.5157876963467225\n",
            "mre:  0.6019338696865401\n",
            "ky 353.50369415283205 0.5157876963467225 60.193386968654\t\n",
            "mae:  424.34325358072914\n",
            "nmae:  0.5580526743565613\n",
            "mre:  51.05789658518099\n",
            "la 424.34325358072914 0.5580526743565613 5105.789658518099\t\n",
            "mae:  424.81277108192444\n",
            "nmae:  6.630792472662712\n",
            "mre:  7.936542844101491\n",
            "ma 424.81277108192444 6.630792472662712 793.6542844101491\t\n",
            "mae:  236.6703955332438\n",
            "nmae:  0.40184005127609457\n",
            "mre:  0.418597877765963\n",
            "md 236.6703955332438 0.40184005127609457 41.8597877765963\t\n",
            "mae:  24.461598745981853\n",
            "nmae:  1.0936631332033615\n",
            "mre:  1.5477068162263277\n",
            "me 24.461598745981853 1.0936631332033615 154.77068162263276\t\n",
            "mae:  292.02968928019203\n",
            "nmae:  0.39554339601813904\n",
            "mre:  1.0493484701801123\n",
            "mi 292.02968928019203 0.39554339601813904 104.93484701801124\t\n",
            "mae:  288.1006561279297\n",
            "nmae:  0.4171140236396839\n",
            "mre:  0.44219629053469095\n",
            "mn 288.1006561279297 0.4171140236396839 44.21962905346909\t\n",
            "mae:  378.7912747701009\n",
            "nmae:  0.3058962083260122\n",
            "mre:  0.30119454866080625\n",
            "mo 378.7912747701009 0.3058962083260122 30.119454866080623\t\n",
            "mae:  327.8043566385905\n",
            "nmae:  0.4866935909708856\n",
            "mre:  0.5836644233803838\n",
            "ms 327.8043566385905 0.4866935909708856 58.36644233803838\t\n",
            "mae:  82.62184270222981\n",
            "nmae:  0.716996031549579\n",
            "mre:  0.6724114292948513\n",
            "mt 82.62184270222981 0.716996031549579 67.24114292948514\t\n",
            "mae:  520.5570753733317\n",
            "nmae:  0.3640259268344977\n",
            "mre:  0.4032623918924905\n",
            "nc 520.5570753733317 0.3640259268344977 40.32623918924905\t\n",
            "mae:  134.80732453664143\n",
            "nmae:  0.6443944767525881\n",
            "mre:  0.618709545607916\n",
            "nd 134.80732453664143 0.6443944767525881 61.8709545607916\t\n",
            "mae:  160.20203132629393\n",
            "nmae:  0.5973972578979264\n",
            "mre:  0.6637903441206728\n",
            "ne 160.20203132629393 0.5973972578979264 66.37903441206728\t\n",
            "mae:  20.965814304351806\n",
            "nmae:  0.948679380287412\n",
            "mre:  1.4379597598324378\n",
            "nh 20.965814304351806 0.948679380287412 143.79597598324378\t\n",
            "mae:  179.00877876281737\n",
            "nmae:  0.5466473292838478\n",
            "mre:  1.3761922810030105\n",
            "nj 179.00877876281737 0.5466473292838478 137.61922810030106\t\n",
            "mae:  145.69830061594647\n",
            "nmae:  1.1306127828449026\n",
            "mre:  1.2457303513254543\n",
            "nm 145.69830061594647 1.1306127828449026 124.57303513254543\t\n",
            "mae:  572.0255709966024\n",
            "nmae:  1.0801766935165904\n",
            "mre:  1.0580618285794328\n",
            "nv 572.0255709966024 1.0801766935165904 105.80618285794327\t\n",
            "mae:  179.34795742034913\n",
            "nmae:  0.27256528483335735\n",
            "mre:  0.27985507110314495\n",
            "ny 179.34795742034913 0.27256528483335735 27.985507110314494\t\n",
            "mae:  333.03291579215755\n",
            "nmae:  0.32239391654613514\n",
            "mre:  0.3301689721275066\n",
            "oh 333.03291579215755 0.32239391654613514 33.01689721275066\t\n",
            "mae:  376.2009117126465\n",
            "nmae:  0.5251269007714217\n",
            "mre:  0.5670664290645069\n",
            "ok 376.2009117126465 0.5251269007714217 56.70664290645069\t\n",
            "mae:  119.17941570281982\n",
            "nmae:  0.5047836327946625\n",
            "mre:  0.5638392294947309\n",
            "or 119.17941570281982 0.5047836327946625 56.38392294947309\t\n",
            "mae:  242.18191922505696\n",
            "nmae:  0.33207448131778\n",
            "mre:  0.3126955513704418\n",
            "pa 242.18191922505696 0.33207448131778 31.26955513704418\t\n",
            "mae:  91.74119647343953\n",
            "nmae:  0.9643433406458255\n",
            "mre:  13.433354413739853\n",
            "ri 91.74119647343953 0.9643433406458255 1343.3354413739853\t\n",
            "mae:  556.8008961995442\n",
            "nmae:  0.6557542058644968\n",
            "mre:  0.7268774558467311\n",
            "sc 556.8008961995442 0.6557542058644968 72.68774558467311\t\n",
            "mae:  116.73654657999674\n",
            "nmae:  0.6040180057605903\n",
            "mre:  0.5771130353475601\n",
            "sd 116.73654657999674 0.6040180057605903 57.711303534756006\t\n",
            "mae:  578.4447530110677\n",
            "nmae:  0.39992032149548373\n",
            "mre:  0.39854852838725063\n",
            "tn 578.4447530110677 0.39992032149548373 39.854852838725066\t\n",
            "mae:  1800.4894652048747\n",
            "nmae:  0.33751794267595364\n",
            "mre:  0.3580878258104866\n",
            "tx 1800.4894652048747 0.33751794267595364 35.80878258104866\t\n",
            "mae:  168.58342787424723\n",
            "nmae:  0.44780439492008295\n",
            "mre:  0.48491789610604197\n",
            "ut 168.58342787424723 0.44780439492008295 48.491789610604194\t\n",
            "mae:  557.6539768095939\n",
            "nmae:  0.5478631324427145\n",
            "mre:  0.5492652286133952\n",
            "va 557.6539768095939 0.5478631324427145 54.92652286133952\t\n",
            "mae:  11.460242144266765\n",
            "nmae:  1.7631141760410407\n",
            "mre:  2.811996503482899\n",
            "vt 11.460242144266765 1.7631141760410407 281.19965034828994\t\n",
            "mae:  159.07068875630696\n",
            "nmae:  0.30203295333476005\n",
            "mre:  0.3485703929125677\n",
            "wa 159.07068875630696 0.30203295333476005 34.85703929125677\t\n",
            "mae:  279.442786026001\n",
            "nmae:  0.3661622005145241\n",
            "mre:  0.3549372544266423\n",
            "wi 279.442786026001 0.3661622005145241 35.49372544266423\t\n",
            "mae:  126.534246190389\n",
            "nmae:  0.9569012820044542\n",
            "mre:  1.0143100906101357\n",
            "wv 126.534246190389 0.9569012820044542 101.43100906101357\t\n",
            "mae:  16.606317456563314\n",
            "nmae:  0.47311445745194625\n",
            "mre:  1.1859461672995997\n",
            "wy 16.606317456563314 0.47311445745194625 118.59461672995997\t\n",
            "global level 354.11141314911646 0.42362447195822206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCWMbZs391Sa",
        "outputId": "6485f23f-f2e2-40bb-fa16-bfb38d1dd080"
      },
      "source": [
        "sum_support = 0\n",
        "sum_error = 0\n",
        "for tpl in gdbt_statewise_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "\n",
        "state_model_global_error = sum_error/sum_support\n",
        "sum_error = 0\n",
        "sum_support = 0\n",
        "for tpl in gdbt_statewise_n_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "state_model_global_error_norm = sum_error/sum_support\n",
        "\n",
        "print('global level', state_model_global_error,state_model_global_error_norm)\n",
        "lis = statewise_dl_errors\n",
        "\n",
        "sorted(lis, key=lambda x: x[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "global level 354.11141314911646 0.42362447195822206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['ak', 34.93873430553236, 0.4731546342160476, 70.96196168140733]],\n",
              " [['al', 555.7893218994141, 0.5064503916401548, 51.975466836200965]],\n",
              " [['ar', 426.6389475370708, 0.7947196081572888, 76.71419284304777]],\n",
              " [['az', 433.0500231291118, 0.6046406848510527, 94.28599901392623]],\n",
              " [['ca', 1742.101870888158, 0.24492164376688003, 20.804268261237787]],\n",
              " [['co', 124.18434102911698, 0.371049296989027, 37.8524632022131]],\n",
              " [['dc', 27.97583096822103, 0.5216127588854138, 51.921273350545924]],\n",
              " [['de', 63.467776711781816, 0.6966825105574295, 113.03187162117551]],\n",
              " [['fl', 1304.4033880869547, 0.33364682607138657, 35.022123870079994]],\n",
              " [['ga', 721.2096433003743, 0.31568772048690824, 39.10276855212912]],\n",
              " [['hi', 278.16956186294556, 1.2539574539276284, 104.10807992566411]],\n",
              " [['ia', 338.92837041219076, 0.460104579952293, 42.118432964317584]],\n",
              " [['id', 179.01713180541992, 0.5557237121443085, 66.17865262302561]],\n",
              " [['il', 836.6678526560465, 0.4230438141252848, 37.84493191986497]],\n",
              " [['in', 350.3296829223633, 0.39148813557591067, 41.34231032481529]],\n",
              " [['ks', 439.7833652496338, 0.7884719391315971, 261.627517877736]],\n",
              " [['ky', 353.50369415283205, 0.5157876963467225, 60.193386968654]],\n",
              " [['la', 424.34325358072914, 0.5580526743565613, 5105.789658518099]],\n",
              " [['ma', 424.81277108192444, 6.630792472662712, 793.6542844101491]],\n",
              " [['md', 236.6703955332438, 0.40184005127609457, 41.8597877765963]],\n",
              " [['me', 24.461598745981853, 1.0936631332033615, 154.77068162263276]],\n",
              " [['mi', 292.02968928019203, 0.39554339601813904, 104.93484701801124]],\n",
              " [['mn', 288.1006561279297, 0.4171140236396839, 44.21962905346909]],\n",
              " [['mo', 378.7912747701009, 0.3058962083260122, 30.119454866080623]],\n",
              " [['ms', 327.8043566385905, 0.4866935909708856, 58.36644233803838]],\n",
              " [['mt', 82.62184270222981, 0.716996031549579, 67.24114292948514]],\n",
              " [['nc', 520.5570753733317, 0.3640259268344977, 40.32623918924905]],\n",
              " [['nd', 134.80732453664143, 0.6443944767525881, 61.8709545607916]],\n",
              " [['ne', 160.20203132629393, 0.5973972578979264, 66.37903441206728]],\n",
              " [['nh', 20.965814304351806, 0.948679380287412, 143.79597598324378]],\n",
              " [['nj', 179.00877876281737, 0.5466473292838478, 137.61922810030106]],\n",
              " [['nm', 145.69830061594647, 1.1306127828449026, 124.57303513254543]],\n",
              " [['nv', 572.0255709966024, 1.0801766935165904, 105.80618285794327]],\n",
              " [['ny', 179.34795742034913, 0.27256528483335735, 27.985507110314494]],\n",
              " [['oh', 333.03291579215755, 0.32239391654613514, 33.01689721275066]],\n",
              " [['ok', 376.2009117126465, 0.5251269007714217, 56.70664290645069]],\n",
              " [['or', 119.17941570281982, 0.5047836327946625, 56.38392294947309]],\n",
              " [['pa', 242.18191922505696, 0.33207448131778, 31.26955513704418]],\n",
              " [['ri', 91.74119647343953, 0.9643433406458255, 1343.3354413739853]],\n",
              " [['sc', 556.8008961995442, 0.6557542058644968, 72.68774558467311]],\n",
              " [['sd', 116.73654657999674, 0.6040180057605903, 57.711303534756006]],\n",
              " [['tn', 578.4447530110677, 0.39992032149548373, 39.854852838725066]],\n",
              " [['tx', 1800.4894652048747, 0.33751794267595364, 35.80878258104866]],\n",
              " [['ut', 168.58342787424723, 0.44780439492008295, 48.491789610604194]],\n",
              " [['va', 557.6539768095939, 0.5478631324427145, 54.92652286133952]],\n",
              " [['vt', 11.460242144266765, 1.7631141760410407, 281.19965034828994]],\n",
              " [['wa', 159.07068875630696, 0.30203295333476005, 34.85703929125677]],\n",
              " [['wi', 279.442786026001, 0.3661622005145241, 35.49372544266423]],\n",
              " [['wv', 126.534246190389, 0.9569012820044542, 101.43100906101357]],\n",
              " [['wy', 16.606317456563314, 0.47311445745194625, 118.59461672995997]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C1Ri0QhDuD0"
      },
      "source": [
        "## Statewise training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfeDsBgXDw19"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def get_mae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    abs_error = abs_sum/len(y_pred)\n",
        "    print('mae: ', abs_error)\n",
        "    return abs_error\n",
        "\n",
        "def get_nmae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    n_abs_error = abs_sum/sum(y_true)\n",
        "    print('nmae: ', n_abs_error)\n",
        "    return n_abs_error\n",
        "\n",
        "def get_mre(y_pred, y_true):\n",
        "    rel_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff= y_pred[j][0]-y_true[j]\n",
        "        rel = diff/(y_true[j] +1)\n",
        "        rel_sum += abs(rel)\n",
        "    rel_error = rel_sum/len(y_pred)\n",
        "    print('mre: ', rel_error)\n",
        "    return rel_error*100\n",
        "\n",
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, input_shape,  verbose=True, build=True):\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, )\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "      \n",
        "            self.model.save_weights('model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        n_feature_maps = 64 * 2\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_y)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        # r= 4 worked better\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=1, padding='same')(conv_y_1)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=3, padding='same')(conv_y_2)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=5, padding='same')(conv_y_3)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.add([conv_y_1, conv_y_2, conv_y_3])\n",
        "\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(128, activation='linear')(gap_layer)\n",
        "\n",
        "        output_layer = keras.layers.Dense(1, activation='linear')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='mae', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "        file_path =  'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(file_path, verbose=0, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "        #                                                   save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit_predict(self, x_train, y_train, x_val, y_val):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = 128\n",
        "        nb_epochs = 50\n",
        "\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save( 'final_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        start_time = time.time()\n",
        "        model_path =  'best_model.hdf5'\n",
        "        # model_path = '/content/model_045_204.676987_362.147278.h5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "merged = merged.sort_values(by='date', ascending=True)\n",
        "X_train = merged[:len(merged)*80//100]\n",
        "X_test = merged[len(merged)*80//100:]\n",
        "X_train = X_train.sample(frac=1)\n",
        "y_train = X_train['daily_case']\n",
        "X_test = X_test.sample(frac=1)\n",
        "y_test = X_test['daily_case']\n",
        "print(len(X_train), len(y_test))\n",
        "\n",
        "# X_train = X_train[features_ranked]\n",
        "# X_test = X_test[features_ranked]\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model= Classifier_RESNET(input_shape=(1, 35))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8vZ345ED-9K"
      },
      "source": [
        "gdbt_statewise_abs_error_list = []\n",
        "gdbt_statewise_n_abs_error_list = []\n",
        "feature_importance_list_gdbt = []\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "statewise_dl_errors = []\n",
        "states = list(merged['state_code'].unique())\n",
        "\n",
        "for state in states:\n",
        "    X_train_state = X_train[X_train['state_code'] ==state][features_ranked]\n",
        "    y_train_state =  X_train[X_train['state_code'] ==state]['daily_case']\n",
        "    X_test_state = X_test[X_test['state_code'] ==state][features_ranked]\n",
        "    y_test_state =  X_test[X_test['state_code'] ==state]['daily_case']   \n",
        "\n",
        "    X_train_state = np.expand_dims(X_train_state, axis=1)\n",
        "    X_test_state = np.expand_dims(X_test_state, axis=1)\n",
        "\n",
        "    # reg = GradientBoostingRegressor() \n",
        "    reg = Classifier_RESNET(input_shape=(1, 35), verbose=False)\n",
        "    reg.fit_predict(X_train_state,y_train_state, X_test_state, y_test_state)    \n",
        "    y_pred= reg.predict(X_test_state)\n",
        "    yp_l=list(y_pred)\n",
        "    yt_l=list(y_test_state)\n",
        "\n",
        "    mae  = get_mae(yp_l,yt_l)\n",
        "    nmae = get_nmae(yp_l,yt_l) \n",
        "    mre  = get_mre(yp_l,yt_l)    \n",
        "\n",
        "    print(state,mae,nmae,mre, end ='\\t') \n",
        "    statewise_dl_errors.append([[state, mae, nmae, mre]])\n",
        "\n",
        "    print ()    \n",
        "    gdbt_statewise_abs_error_list.append((state,mae,len(y_test_state)))\n",
        "    gdbt_statewise_n_abs_error_list.append((state,nmae,sum(y_test_state)))    \n",
        "    # importances = reg.feature_importances_\n",
        "    # for i in range(len(features_ranked)):\n",
        "    #    feature_importance_list_gdbt.append([state,features_ranked[i],importances[i]])sum_error = 0\n",
        "\n",
        "sum_support = 0\n",
        "sum_error = 0\n",
        "for tpl in gdbt_statewise_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "\n",
        "state_model_global_error = sum_error/sum_support\n",
        "sum_error = 0\n",
        "sum_support = 0\n",
        "for tpl in gdbt_statewise_n_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "state_model_global_error_norm = sum_error/sum_support\n",
        "\n",
        "print('global level', state_model_global_error,state_model_global_error_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6D3qwtG-Een"
      },
      "source": [
        "# CNN Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjXhhTBu-F30"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "NB_EPOCHS = 1000  # num of epochs to test for\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "## Create our model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer: input_dim=8, 12 nodes, RELU\n",
        "model.add(Conv1D(32, 1, activation='relu',input_shape=(1, 35)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(4, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(32, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(4, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(16, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(4, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(2, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(24, activation='relu'))\n",
        "# 2nd layer: 8 nodes, RELU\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(8,  activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# output layer: dim=1, activation sigmoid\n",
        "model.add(Dense(1,  activation='linear' ))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_absolute_error',   \n",
        "             optimizer='adam',\n",
        "             metrics=['mae'])\n",
        "\n",
        "# checkpoint: store the best model\n",
        "ckpt_model = 'pima-weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(ckpt_model, \n",
        "                            monitor='val_mae',\n",
        "                            verbose=0,\n",
        "                            save_best_only=True,\n",
        "                            mode='max')\n",
        "file_path =  'best_model.hdf5'\n",
        "\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(file_path, \n",
        "                                                   verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "callbacks_list = [model_checkpoint]\n",
        "\n",
        "print('Starting training...')\n",
        "\n",
        "merged = merged.sort_values(by='date', ascending=True)\n",
        "X_train = merged[:len(merged)*80//100]\n",
        "X_test = merged[len(merged)*80//100:]\n",
        "X_train = X_train.sample(frac=1)\n",
        "y_train = X_train['daily_case']\n",
        "X_test = X_test.sample(frac=1)\n",
        "y_test = X_test['daily_case']\n",
        "print(len(X_train), len(y_test))\n",
        "\n",
        "X_train = X_train[features_ranked]\n",
        "X_test = X_test[features_ranked]\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "\n",
        "# train the model, store the results for plotting\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=1000,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks_list,\n",
        "                    verbose=1)\n",
        "\n",
        "# def print_metrics(y_pred, y_gt):\n",
        "\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "y_pred = model.predict(X_test)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_test.tolist()\n",
        "abs_sum = 0\n",
        "pct_sum = 0\n",
        "for idx in range(len(y_pred)):\n",
        "    diff = abs(yp_l[idx][0][0] - yt_l[idx])\n",
        "    abs_sum += diff\n",
        "    pct = diff/ (yt_l[idx] + 1)\n",
        "    pct_sm += pct\n",
        "\n",
        "rel_error = pct_sm / len(y_pred)\n",
        "abs_error = abs_sum / len(y_pred)\n",
        "gb_rel_errors.append(rel_error * 100)\n",
        "gb_abs_errors.append(abs_error)\n",
        "\n",
        "print('Test: ', abs_error,rel_error * 100)\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_train.tolist()\n",
        "\n",
        "abs_sum = 0\n",
        "pct_sum = 0\n",
        "for idx in range(len(y_pred)):\n",
        "    diff = abs(yp_l[idx][0][0] - yt_l[idx])\n",
        "    abs_sum += diff\n",
        "    pct = diff/ (yt_l[idx] + 1)\n",
        "    pct_sm += pct\n",
        "\n",
        "rel_error = pct_sm / len(y_pred)\n",
        "abs_error = abs_sum / len(y_pred)\n",
        "gb_rel_errors.append(rel_error * 100)\n",
        "gb_abs_errors.append(abs_error)\n",
        "\n",
        "print('Train: ', abs_error,rel_error * 100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkpxyGGn99CD"
      },
      "source": [
        "def get_mae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    abs_error = abs_sum/len(y_pred)\n",
        "    print('mae: ', abs_error)\n",
        "    return abs_error\n",
        "\n",
        "def get_nmae(y_pred, y_true):\n",
        "    abs_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff=abs(y_pred[j][0][0]-y_true[j])\n",
        "        abs_sum += diff\n",
        "    n_abs_error = abs_sum/sum(y_true)\n",
        "    print('nmae: ', n_abs_error)\n",
        "    return n_abs_error\n",
        "\n",
        "def get_mre(y_pred, y_true):\n",
        "    rel_sum = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        diff= y_pred[j][0][0]-y_true[j]\n",
        "        rel = diff/(y_true[j] +1)\n",
        "        rel_sum += abs(rel)\n",
        "    rel_error = rel_sum/len(y_pred)\n",
        "    print('mre: ', rel_error)\n",
        "    return rel_error*100\n",
        "    \n",
        "print('Test: ')\n",
        "# model_path = '/content/cnn_model_885_407.555481_459.086090.h5'\n",
        "# model_path = 'cnn_model_283_420.091034_462.834656.h5'\n",
        "model_path = file_path\n",
        "model = keras.models.load_model(model_path)\n",
        "y_pred = model.predict(X_test)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_test.tolist()\n",
        "\n",
        "get_mae(yp_l, yt_l)\n",
        "get_nmae(yp_l, yt_l)\n",
        "get_mre(yp_l, yt_l)\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "yp_l = y_pred.tolist()\n",
        "yt_l = y_train.tolist()\n",
        "\n",
        "print('Train: ')\n",
        "get_mae(yp_l, yt_l)\n",
        "get_nmae(yp_l, yt_l)\n",
        "get_mre(yp_l, yt_l)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaVreiWJ-kgN"
      },
      "source": [
        "### state wise training (local models)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD9UTJKp3ehC"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "\n",
        "gdbt_statewise_abs_error_list = []\n",
        "gdbt_statewise_n_abs_error_list = []\n",
        "feature_importance_list_gdbt = []\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "statewise_dl_errors = []\n",
        "states = list(merged['state_code'].unique())\n",
        "states.sort()\n",
        "\n",
        "merged = merged.sort_values(by='date', ascending=True)\n",
        "X_train = merged[:len(merged)*80//100]\n",
        "X_test = merged[len(merged)*80//100:]\n",
        "X_train = X_train.sample(frac=1)\n",
        "y_train = X_train['daily_case']\n",
        "X_test = X_test.sample(frac=1)\n",
        "y_test = X_test['daily_case']\n",
        "print(len(X_train), len(y_test))\n",
        "\n",
        "# X_train = X_train[features_ranked]\n",
        "# X_test = X_test[features_ranked]\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "def train_cnn(X_train_state, y_train_state, X_test_state, y_test_state):\n",
        "    ## Create our model\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1st layer: input_dim=8, 12 nodes, RELU\n",
        "    model.add(Conv1D(32, 1, activation='relu',input_shape=(1, 35)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(4, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(32, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(4, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(16, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(4, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(2, 1, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    # 2nd layer: 8 nodes, RELU\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8,  activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    # output layer: dim=1, activation sigmoid\n",
        "    model.add(Dense(1,  activation='linear' ))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_absolute_error',   \n",
        "                optimizer='adam',\n",
        "                metrics=['mae'])\n",
        "\n",
        "    # checkpoint: store the best model\n",
        "    \"\"\"\n",
        "    ckpt_model = 'pima-weights.best.hdf5'\n",
        "    checkpoint = ModelCheckpoint(ckpt_model, \n",
        "                                monitor='val_mae',\n",
        "                                verbose=0,\n",
        "                                save_best_only=True,\n",
        "                                mode='max')\n",
        "    file_path =  'best_model.hdf5'\n",
        "    \"\"\"\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint('best_model.hdf5', \n",
        "                                                    verbose=0, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "    callbacks_list = [model_checkpoint]\n",
        "\n",
        "    # print('Starting training...')\n",
        "\n",
        "    # train the model, store the results for plotting\n",
        "    history = model.fit(X_train_state,\n",
        "                        y_train_state,\n",
        "                        validation_data=(X_test_state, y_test_state),\n",
        "                        epochs=100,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        callbacks=callbacks_list,\n",
        "                        verbose=0)\n",
        "    return model\n",
        "\n",
        "for state in states:\n",
        "    X_train_state = X_train[X_train['state_code'] ==state][features_ranked]\n",
        "    y_train_state =  X_train[X_train['state_code'] ==state]['daily_case']\n",
        "    X_test_state = X_test[X_test['state_code'] ==state][features_ranked]\n",
        "    y_test_state =  X_test[X_test['state_code'] ==state]['daily_case']   \n",
        "\n",
        "    X_train_state = np.expand_dims(X_train_state, axis=1)\n",
        "    X_test_state = np.expand_dims(X_test_state, axis=1)\n",
        "\n",
        "    # reg = GradientBoostingRegressor() \n",
        "    # reg = Classifier_RESNET(input_shape=(1, 35), verbose=False)\n",
        "    train_cnn(X_train_state, y_train_state, X_test_state, y_test_state)\n",
        "\n",
        "    \n",
        "    model_path = 'best_model.hdf5'\n",
        "    reg = keras.models.load_model(model_path)\n",
        "    # reg.fit_predict(X_train_state,y_train_state, X_test_state, y_test_state)    \n",
        "    y_pred= reg.predict(X_test_state)\n",
        "    yp_l=list(y_pred)\n",
        "    yt_l=list(y_test_state)\n",
        "\n",
        "    mae  = get_mae(yp_l,yt_l)\n",
        "    nmae = get_nmae(yp_l,yt_l) \n",
        "    mre  = get_mre(yp_l,yt_l)    \n",
        "\n",
        "    print(state,mae,nmae,mre, end ='\\t') \n",
        "    statewise_dl_errors.append([[state, mae, nmae, mre]])\n",
        "\n",
        "    print ()    \n",
        "    gdbt_statewise_abs_error_list.append((state,mae,len(y_test_state)))\n",
        "    gdbt_statewise_n_abs_error_list.append((state,nmae,sum(y_test_state)))    \n",
        "    # importances = reg.feature_importances_\n",
        "    # for i in range(len(features_ranked)):\n",
        "    #    feature_importance_list_gdbt.append([state,features_ranked[i],importances[i]])sum_error = 0\n",
        "\n",
        "sum_support = 0\n",
        "sum_error = 0\n",
        "for tpl in gdbt_statewise_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "\n",
        "state_model_global_error = sum_error/sum_support\n",
        "sum_error = 0\n",
        "sum_support = 0\n",
        "for tpl in gdbt_statewise_n_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "state_model_global_error_norm = sum_error/sum_support\n",
        "\n",
        "print('global level', state_model_global_error,state_model_global_error_norm)\n",
        "\n",
        "sum_support = 0\n",
        "sum_error = 0\n",
        "for tpl in gdbt_statewise_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "\n",
        "state_model_global_error = sum_error/sum_support\n",
        "sum_error = 0\n",
        "sum_support = 0\n",
        "for tpl in gdbt_statewise_n_abs_error_list: \n",
        "    sum_error += tpl[1] * tpl[2]\n",
        "    sum_support += tpl[2]\n",
        "state_model_global_error_norm = sum_error/sum_support\n",
        "\n",
        "print('global level', state_model_global_error,state_model_global_error_norm)\n",
        "lis = statewise_dl_errors\n",
        "\n",
        "sorted(lis, key=lambda x: x[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWYM6sNg-sjc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6WBeUNh3QAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpUDN4X3KIw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}